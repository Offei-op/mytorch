{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba603181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06cf13f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words =  open('names.txt','r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9736e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(alphabets)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c017e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Dataset\n",
    "\n",
    "def build_dataset(words):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    len_ws =[] \n",
    "\n",
    "    for w in words:\n",
    "\n",
    "        chr = '...' + w +'.'\n",
    "        i = 0\n",
    "\n",
    "        for chr1,chr2,chr3,chr4 in zip(chr,chr[1:],chr[2:],chr[3:]):\n",
    "            len_ws.append(i)\n",
    "            i += 1\n",
    "            xs.append([stoi[chr1],stoi[chr2],stoi[chr3]])\n",
    "            ys.append(stoi[chr4])\n",
    "    len_ws = torch.tensor(len_ws)\n",
    "    len_ws = F.one_hot(len_ws,num_classes=16)\n",
    "\n",
    "\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "\n",
    "    return xs,ys,len_ws\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(words)\n",
    "\n",
    "train_ix = int(len(words) * 0.8)\n",
    "val_ix = int(len(words) * 0.9)\n",
    "\n",
    "Xtr,ytr,nth_tr = build_dataset(words[:train_ix])\n",
    "Xval,yval,nth_val = build_dataset(words[train_ix:val_ix])\n",
    "Xte,yte,nth_te = build_dataset(words[val_ix:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e42a227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([182418, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ee75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vec_dim = 10\n",
    "h_size = 300\n",
    "\n",
    "g = torch.random.manual_seed(576809309)\n",
    "\n",
    "C = torch.randn((27,feature_vec_dim),generator=g);  C.requires_grad = True\n",
    "W1 = torch.randn((46,h_size),generator=g);          W1.requires_grad = True\n",
    "W2 = torch.randn((h_size,27),generator=g) * 0.01;          W2.requires_grad = True\n",
    "b1 = torch.randn((1,h_size),generator=g);           b1.requires_grad = True\n",
    "b2 = torch.randn((1,27),generator=g) * 0;               b2.requires_grad = True\n",
    "\n",
    "parameters = [C,W1,W2,b1,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97420347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strike 1: 1.9527976512908936 => 1.9536689519882202\n",
      "Strike 2: 1.9461408853530884 => 1.94944167137146\n",
      "Strike 3: 1.94944167137146 => 1.9486933946609497\n",
      "Loss on 135000th iteration => 1.94944167137146\n",
      "Loss on 136000th iteration => 1.9486933946609497\n",
      "Reducing learning rate from 0.1 to 0.05\n",
      "Strike 1: 1.935076117515564 => 1.936283826828003\n",
      "Strike 2: 1.936283826828003 => 1.9357365369796753\n",
      "Strike 3: 1.9322268962860107 => 1.9322967529296875\n",
      "Loss on 185000th iteration => 1.9322268962860107\n",
      "Loss on 186000th iteration => 1.9322967529296875\n",
      "Reducing learning rate from 0.05 to 0.025\n",
      "Loss is reducing at too slow a rate. Training has been halted\n"
     ]
    }
   ],
   "source": [
    "running_losses = []\n",
    "lr = 0.1\n",
    "patience = 3\n",
    "\n",
    "strikes = 0\n",
    "\n",
    "for i in range(300000):\n",
    "\n",
    "    if i % 1000 != 0:\n",
    "\n",
    "        ix = torch.randint(0,len(Xtr),(512,))\n",
    "        #forward pass\n",
    "        X_enc = C[Xtr[ix]].view((-1,3 * feature_vec_dim))\n",
    "        X_enc = torch.cat([X_enc,nth_tr[ix]],dim=1)\n",
    "        act = X_enc @ W1 + b1\n",
    "        h = act.tanh()\n",
    "        logits = h @ W2 + b2\n",
    "        loss = F.cross_entropy(logits,ytr[ix])\n",
    "\n",
    "        if i%10000 == 0:\n",
    "            print(f\"Loss {i} ----> {loss.item()}\")\n",
    "\n",
    "        #Backward Pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "\n",
    "        loss.backward()\n",
    "        #Update Parameters \n",
    "        for p in parameters:\n",
    "            p.data -= lr * p.grad\n",
    "    else:\n",
    "        #Evaluate on full dataset\n",
    "        X_enc = C[Xtr].view((-1,3 * feature_vec_dim))\n",
    "        X_enc = torch.cat([X_enc,nth_tr],dim=1)\n",
    "        act = X_enc @ W1 + b1\n",
    "        h = act.tanh()\n",
    "        logits = h @ W2 + b2\n",
    "        loss = F.cross_entropy(logits,ytr)\n",
    "        running_losses.append(loss.item())\n",
    "        if  len(running_losses) > 9 and running_losses[-1] > running_losses[-10]: # Looking 10000 runs back for a drop in loss...\n",
    "            print(f\"Strike {strikes + 1}: {running_losses[-2]} => {running_losses[-1]}\")\n",
    "            strikes += 1\n",
    "        if strikes == patience:\n",
    "            strikes = 0\n",
    "            print(f\"Loss on {i-1000}th iteration => {running_losses[-2]}\")\n",
    "            print(f\"Loss on {i}th iteration => {running_losses[-1]}\")\n",
    "            print(f\"Reducing learning rate from {lr} to {lr*0.5}\")\n",
    "            lr *= 0.5 # Halve the learning rate\n",
    "\n",
    "        if len(running_losses) > 49 and running_losses[-50] - running_losses[-1] < 0.01: #Exploring over every 10,000 steps\n",
    "            print(\"Loss is reducing at too slow a rate. Training has been halted\")\n",
    "            break\n",
    "    \n",
    "    if lr < 0.005:\n",
    "        print(\"Gradient is vanishing...\")\n",
    "        break #Halt before gradient starts to vanish   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26cb76c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186000\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba0a07d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9322967529296875\n"
     ]
    }
   ],
   "source": [
    "X_enc = C[Xtr].view((-1,3 * feature_vec_dim))\n",
    "X_enc = torch.cat([X_enc,nth_tr],dim=1)\n",
    "act = X_enc @ W1 + b1\n",
    "h = act.tanh()\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits,ytr)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89ccd49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0584189891815186\n"
     ]
    }
   ],
   "source": [
    "X_enc = C[Xte].view((-1,3 * feature_vec_dim))\n",
    "X_enc = torch.cat([X_enc,nth_te],dim=1)\n",
    "act = X_enc @ W1 + b1\n",
    "h = act.tanh()\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits,yte)\n",
    "print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
